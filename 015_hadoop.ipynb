{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `hadoop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*soon we will need the following file, and it takes a considerable amount of time to download. start downloading it now*\n",
    "\n",
    "https://resources.oreilly.com/examples/0636920035275/raw/master/hfpd3.vmdk.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the problem(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "reach all the way back in your memory several lectures ago, when we talked about `aws` `dynamodb`. the proposed use case for `dynamodb` was an ambiguous \"webby\" one:\n",
    "\n",
    "*we're reading and writing way too much data way too fast for our one machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this sentiment is reflective of a modern data reality that often goes by the buzziest of buzzwords:\n",
    "\n",
    "**big data**\n",
    "\n",
    "*mandatory caveat: big data $\\neq$ data science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "traditional data analyses were optimized for super-powerful single machines such as the monolithic, super-powerful `sql` servers\n",
    "\n",
    "*note: this is not to say that cluster computing didn't exist; indeed it was one of the main computational frameworks in the early days of computers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "our exponential growth in disk space and memory space per dollar spent fueled a lot of this work and innovation.\n",
    "\n",
    "basically, for a long while our ability to *compute* data grew faster than our ability to *create* or *acquire* data. in the modern world, though, that notion is absolute history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "take, for example, a relatively trivial process for modern computation: word counts for a document of several MBs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm /tmp/shakespeare.txt\n",
    "wget --quiet -O /tmp/shakespeare.txt.zip https://github.com/bbengfort/hadoop-fundamentals/raw/master/data/shakespeare.txt.zip\n",
    "unzip /tmp/shakespeare.txt.zip -d /tmp\n",
    "ls -alh /tmp/shak*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head /tmp/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with open('/tmp/shakespeare.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "\n",
    "print(s[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "wordct = collections.Counter(\n",
    "    word.lower()\n",
    "    for line in s.split('\\n')\n",
    "    for word in line.strip().split('\\t')\n",
    "    if word\n",
    ")\n",
    "\n",
    "wordct.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that was easy, but it relied on some important features:\n",
    "\n",
    "1. I had enough disk space to have that 8.5MB file stored locally\n",
    "2. I had enough memory to load that 8.5MB file's contents directly into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "obviously, that isn't always the case. It's not even hard to think of counter-examples\n",
    "\n",
    "1. a larger text corpus (e.g. all of wikipedia, 10TB as of 2015, or publically available SEC EDGAR filings)\n",
    "2. any reasonably large image recognition project\n",
    "3. the logs of web traffic for any modestly sized website or service\n",
    "4. IoT information (usage records of your smartphone or headphones, e.g.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so, back to `dynamodb`. when we (theoretically) started to run into resource issues for our single-machine architecture, we decided to change the way we were doing things and start \"scaling horizontally\" -- choose an architecture and software that can spread the storage and computation burden across multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "for `dynamodb` we were attempting to distribute out our database writes and reads as actions, but the test scenario I laid out above was one of\n",
    "\n",
    "+ data storage\n",
    "+ resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hadoop` is the *de facto* operating system for distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "it is a software solution that abstracts out all the \"hard stuff\" (the complicated networking and resource martialing) that needs to happen to get multiple computers on the same page, and instead provides the user (you) with a single api for\n",
    "\n",
    "+ accessing distributed files (`hdfs`)\n",
    "+ securing computational resources and memory (`yarn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `hadoop` nuts and bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's dig into the details of distributed computing a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### terminology\n",
    "\n",
    "+ **node**: a single machine (real or virtual)\n",
    "+ **cluster**: a collection of *nodes* which can communicate with eachother\n",
    "+ **master**: a *node* which can request information from or delegate tasks to other *nodes*\n",
    "+ **worker**: a *node* which merely receives, processes, and responds to requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### basic concepts\n",
    "\n",
    "in the database world we had certain requirements a `dbms` needed to meet to ensure that all clients of that database service could share those resources (the `ACID` principles)\n",
    "\n",
    "similarly, for distributed computing to be well defined and robust, we have four requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. *fault tolerance*: if one computer goes down, we're good. if it comes back, we're even gooder\n",
    "2. *recoverability*: we don't lose data when things fail\n",
    "3. *consistency*: results shouldn't depend on jobs failing or succeeding\n",
    "4. *scalability*: more data means longer time, not failure; we can increase resources if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hadoop` addresses these requirements by making the following decisions:\n",
    "\n",
    "+ data is distribute across many nodes in the cluster; each node prefers it's local data\n",
    "+ all data is chunked into blocks (say, 128 MB) and is *replicated* (copied to other nodes)\n",
    "+ jobs (computations) are broken into tasks applied to single blocks\n",
    "+ jobs are completely unaware that they are distributed\n",
    "+ worker nodes don't care about eachother\n",
    "+ tasks are redundant and repeatable\n",
    "+ master nodes handle allocation of all resources (storage, cpus, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `hadoop` architecture\n",
    "\n",
    "`hadoop` as an operating system is basically just two pieces of software:\n",
    "\n",
    "1. `hdfs` (a program for handling distributed file storage)\n",
    "2. `yarn` (a program for handling distributed resource allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "together, these two process conspire to enforce some of those design decisions above: namely, to make sure that all data is robustly distributed and that all distributed tasks are working on local data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs` and `yarn` are the defaults and they were built to work in tandem, but either can be replaced:\n",
    "\n",
    "+ you could change stoarge methods (e.g. `hdfs` replaced by `s3`)\n",
    "+ you could change resource managers or computational layers on top of storage (e.g. `yarn` replaced by `hbase`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a `hadoop` cluster\n",
    "\n",
    "`hadoop` is a software. the hardware is a cluster of computers. the benefit you get in using `hdfs` and `yarn` are abstracted `api`s that hide cluster administration details and tasks from you.\n",
    "\n",
    "to put it another way: `hadoop` lets someone else do the hard task of distribution so you can do what you came here to do (analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when we've talked before about databases or `aws` `REST` `api`s, we've often called them *services* and the programs we wrote to utilize those services *clients*\n",
    "\n",
    "both `hdfs` and `yarn` have several *services* that our tools (*clients*) use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs` services:\n",
    "\n",
    "+ `NameNode` (master): stores the directory tree, file metadata, file cluster locations. this is the access point for `hdfs` usage\n",
    "+ `Secondary NameNode` (master): performs housekeeping, checkpointing. not a backup `NameNode`\n",
    "+ `DataNode` (worker): local `io` for `hdfs` blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the basic interaction with `hdfs`:\n",
    "\n",
    "1. client asks `NameNode` where data lives.\n",
    "2. `NameNode` tells client\n",
    "3. client is responsible for going and getting data from `DataNode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`yarn` services:\n",
    "\n",
    "+ `ResourceManager` (master): allocates and monitor resources (memory, cores), schedules jobs\n",
    "+ `ApplicationMaster` (master): coordinates a particular app after `ResourceManager` has scheduled it\n",
    "+ `NodeManager` (worker): runs tasks and reports on task status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the basic interaction with `yarn` is very similar:\n",
    "\n",
    "1. client asks `ResourceManager` for resources\n",
    "2. `ResourceManager` assigns `ApplicationMaster` instance to manage the individual application\n",
    "3. `ApplicationMaster` submits a job to a single `NodeManager`, tracks all submitted jobs\n",
    "4. `NodeManager` executes incoming assigned tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to give you a sense of scale for a typical `hadoop` cluster\n",
    "\n",
    "+ 20 - 30 workers and one master can handle 10s of terrabytes of data in simulatneous workflows\n",
    "+ single servers (resource absolutism) is needed once you have hundreds of nodes\n",
    "+ multiple masters are needed when you start talking about thousands of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### details on `hdfs`\n",
    "\n",
    "`hdfs` is a file system on top of another filesystem. in many respects, it behaves like you're used to the `linux` filesystem behaving (with slightly different commands). there are a few nuances worth discussing, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### blocks\n",
    "\n",
    "files are blocked into large (e.g. 128MB) chunks. this means that a file larger than that will be separated up into different blocks. it's worth noting: this is effectively the *only* sense in which the block size matters\n",
    "\n",
    "a small file will not wastefully take up the remainder of the space on the OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that's not to say there isn't a problem with small files, though -- there is. It's not wasteful disk usage, it's wasteful *resource* usage. we will discuss `mappers` and `reducers` later, but for now it suffices to say: when we distributed tasks, we already said we distribute them to blocks.\n",
    "\n",
    "if one of those blocks contains a small amount of information, that will be pretty wasteful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "better a million files of 100 MB than a billion files of 0.1 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a demo `hadoop` environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "a single `hadoop` (or, related, `aws` `emr`) environment is often a large, complicated, expensive, and unruly engineering project.\n",
    "\n",
    "to avoid the hastle of constantly building up complicated development environments, many developers will create a *virtual execution environment* in a *virtual machine*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we are going to build one such virtual environment right now using oracle's `virtualbox` and the Ubuntu 14.04 `vmdk` provided by the authors of our text book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">walkthrough: installing `virtualbox` and a `hadoop` virtual machine</div>**\n",
    "\n",
    "1. if you didn't start the download of https://resources.oreilly.com/examples/0636920035275/raw/master/hfpd3.vmdk.gz at the beginning of class, do so now\n",
    "2. download `virtualbox` for your os and follow instructions: https://www.virtualbox.org/wiki/Downloads\n",
    "4. unzip the `vmdk` file once it is downloaded\n",
    "    1. in a terminal, `gunzip -k hfpd3.vmdk.gz`\n",
    "5. create the VM\n",
    "    1. open `virtualbox` and click the \"new\" button\n",
    "    2. name it whatever you want, change the type to `linux`, and make the version \"ubuntu 64-bit\"\n",
    "    3. set the memory however you want (I'll go high because yolo)\n",
    "    4. select \"Use an existing virtual hard disk file\" and navigate to the `vmdk` file\n",
    "    5. start up the VM. password is `password`\n",
    "    6. click \"Devices > Insert guest additions cd\" and the run (again, password is `password`)\n",
    "    7. restart the VM when finished (now you can resize!)\n",
    "    8. back in the `virtualbox` program, navigate to \"Settings\", and on the \"General > Advanced\" tab make \"Shared Clipboard\" \"Bidirectional\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "note: [the `cloudera` vm](https://www.cloudera.com/downloads/quickstart_vms/5-12.html) is actually pretty excellent to use and I highly recommend it for your general development and hacking.\n",
    "\n",
    "I opted for the course-specific `vmdk` so that we would avoid configuration and implementation discrepancies as much as is possible, and also because the `cloudera` download requires you provide a lot of identifying information and I am attempting to respect privacy when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">starting `hadoop`</div>**\n",
    "\n",
    "1. log in to your `hadoop` vm\n",
    "2. execute the following:\n",
    "\n",
    "```bash\n",
    "sudo -H -u hadoop $HADOOP_HOME/sbin/start-dfs.sh\n",
    "sudo -H -u hadoop $HADOOP_HOME/sbin/start-yarn.sh\n",
    "sudo -H -u hadoop $HADOOP_HOME/bin/hadoop fs -chown -R hadoop:hadoop /\n",
    "sudo -H -u hadoop $HADOOP_HOME/bin/hadoop fs -chown -R student:student /user/student\n",
    "sudo -H -u hadoop $HADOOP_HOME/bin/hadoop fs -chmod g+w /\n",
    "sudo chmod g+w /var/app/hadoop/data\n",
    "\n",
    "# demonstrate it worked\n",
    "hadoop fs -mkdir -p /user/student\n",
    "hadoop fs -ls -h /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### working with a distributed file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### basic file system operations\n",
    "\n",
    "many of the common `linux` command line file system tools are available with the same names in `hadoop`. try\n",
    "\n",
    "```bash\n",
    "hadoop fs -help\n",
    "```\n",
    "\n",
    "(note the single-dash parameters and curse the `java` gods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tired of reading those 4000 lines? try any one subcommand too:\n",
    "\n",
    "```bash\n",
    "hadoop fs -help ls\n",
    "hadoop fs -help chmod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "out on the etherwebs, you may see floating around commands such as\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs dfs` is *related to* `hadoop fs`, but is not exactly the same. `hadoop fs` defaults to looking at `hdfs` files, but is actually file-system agnostic(ish), and supports local files (via the `file://` schema), `s3` files, `ftp` services, and any other people have been kind enough to implement.\n",
    "\n",
    "`hdfs dfs` *only* works with `hdfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to demonstrate how `hadoop fs` can be used with local files as well, try out\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls file:///tmp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "none of this is to say *prefer* `hadoop fs` or *avoid* `hdfs dfs`. just knowing what the difference is may help you avoid some confusion when you try the subcommands or flags of one and don't experience the same result as you would with the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's prepare our `hadoop` cluster to actually do some `hadoop`-y stuff. on the master node (your virtual machine), run:\n",
    "\n",
    "```bash\n",
    "mkdir ~/code && cd ~/code\n",
    "git clone https://github.com/bbengfort/hadoop-fundamentals.git\n",
    "cd hadoop-fundamentals/data\n",
    "unzip shakespeare.txt.zip\n",
    "hadoop fs -copyFromLocal shakespeare.txt shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the files we save in `hadoop` are generally enormous. it's good to know right away how to read portions of such large files:\n",
    "\n",
    "+ `hadoop fs -cat shakespeare.txt | less`\n",
    "+ `hadoop fs -cat shakespeare.txt | head` (this aborts the streaming when `head` has had enough)\n",
    "+ `hadoop fs -tail shakespeare.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### other `hdfs` interfaces\n",
    "\n",
    "finally, there are `http` integrations. in particular, check out the web interface for the `DataNode`s found at `datanode_url:50075` (for our `hadoop` vm, try `127.0.0.1:50075`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### working with distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "as we said above, `yarn` is the main resource manager and one of the main access points for computation. in the original instance of `hadoop`, however, the computational framework was a software called `mapreduce`\n",
    "\n",
    "knowing what `mapreduce` is helps illuminate the engineering paradigm at play in `hadoop` programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mapreduce`: a functional programming model\n",
    "\n",
    "`mapreduce` was [first proposed](https://research.google.com/archive/mapreduce.html) by google developers as a way of performing easily distributable computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the name comes from the two \"pieces\":\n",
    "\n",
    "+ a `map` function takes input as a series of key-value pairs (\"kvps\") and performs the same computation on each pair, generating a (possibly empty) sequence of intermediate kvps\n",
    "    + this is where analysis happens (usually)\n",
    "    + e.g. filter: take a key, check if it belongs in a list of acceptable keys, emit the kvp if yes, pass silently if no\n",
    "+ a `reduce` function takes a key and an iterator of values and process the values, usually to determine some aggregate statistic\n",
    "\n",
    "these functions ought to be stateless functional programming functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[this simple diagram](https://www.tutorialspoint.com/map_reduce/images/mapreduce_work.jpg) shows a simple prototype mapreduce process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mapreduce`: implemented on a cluster\n",
    "\n",
    "the `mapreduce` framework is great for a distributed computation environment because it is assumes many of the central tenets of the distribution framework. specifically, because mappers and reducers are stateless functions, they can be executed by a worker node to independently work on any number of blocks and emit their responses back to the master node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mappers are already set: individual blocks are key-value pairs where the keys are file or line metadata and the values are the contents of the file / line. we can distribute the mapper function to any number of workers and let them process blocks at their own pace without any outside information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "reducers needs all the output values for a single key across all processed blocks, so we have to wait until all mappers are done to \"reduce\".\n",
    "\n",
    "we create as many reducers as there are output keys and distribute them among the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "because reducers expect to get the keys emitted by mappers and **all** values for those keys, we need to perform a shuffle and sort of those intermediate kvps before we can reduce. this stage is called exactly that: *shuffle and sort*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so, in the end, we have a general framework:\n",
    "\n",
    "+ input: `hdfs` kvps\n",
    "+ mapping: input kvps are processed by mappers and generate intermediate kvps\n",
    "+ shuffle and sort: take the generated key, partition the key space, and assign keys to reducers\n",
    "+ reduce: take the keys and the iterated list of values and reduce them to aggregate kvps\n",
    "\n",
    "it's kvps all the way down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### mapreduce examples\n",
    "\n",
    "we already counted words in the shakespeare corpus, in memory in plain `python`, and the pseudo-code which can fit this wordcount problem into `mapreduce` is not that different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def mapper(documentkey, line):\n",
    "    for word in line.split():\n",
    "        emit(word, 1)\n",
    "        \n",
    "def reducer(word, values):\n",
    "    emit(word, sum(val for val in values))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### submitting a mapreduce job to `yarn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`yarn` is responsible for scheduling tasks, so if we would like to perform some task we need to give it to `yarn`.\n",
    "\n",
    "one way (and the most basic) is to create a `jar` file (compiled `java` code) and to pass that directly to `yarn` using the `hadoop jar` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "in [the github repo](https://github.com/bbengfort/hadoop-fundamentals) for the \"Data Analytics with Hadoop\" O'Reilly book, we have been provided with a couple `java` files to implement a simple `mapreduce` word count job\n",
    "\n",
    "+ [`WordCount.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/WordCount.java)\n",
    "+ [`WordMapper.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/WordMapper.java)\n",
    "+ [`SumReducer.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/SumReducer.java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's compile and run that code on the shakespeare corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "first thing's first, let's compile our `java` code into a `jar` file\n",
    "\n",
    "```bash\n",
    "export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar\n",
    "cd ~/code/hadoop-fundamentals/wordcount/WordCount/\n",
    "hadoop com.sun.tools.javac.Main *.java\n",
    "jar cf wc.jar WordCount.class WordMapper.class SumReducer.class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "second thing's second, let's fix a simple permission problem on our local machine and then another one on our `hdfs`.\n",
    "\n",
    "```bash\n",
    "sudo chmod g+w /var/app/hadoop/data\n",
    "sudo su hadoop\n",
    "hadoop fs -chmod g+w /\n",
    "exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "final thing's final, we can submit the `jar` file to `yarn` by calling\n",
    "\n",
    "```bash\n",
    "hadoop jar wc.jar WordCount shakespeare.txt wordcounts\n",
    "```\n",
    "\n",
    "we can track the results of that job via a web interface at 127.0.0.1:8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## not using `java`  with `hadoop` streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so we were able to write `java` code to create `mapreduce` jobs. super.\n",
    "\n",
    "I mean... not knowing `java` is a bit of a problem though. not to be ungrateful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is just what we get out of the box with `hadoop` `mapreduce`.\n",
    "\n",
    "+ java api with input, output, map and reduce functions, job params exposed as *job configuration*\n",
    "+ jobs get packaged into a jar which is passed to the `ResourceManager` by the *job client*\n",
    "+ `ResourceManager` handles the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "but what if you don't want to write `java` code that implements this same workflow over and over and over again?\n",
    "\n",
    "or just don't want to write `java` code *at all*, because you already did everything you needed to do in `python`?\n",
    "\n",
    "*hadoop streaming* is here to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `hadoop` streaming\n",
    "\n",
    "hadoop streaming is a `java` util which can take any executable (in *any* language!) and use that as a mapper or reducer or combiner.\n",
    "\n",
    "really, this is just a super hacky `jar` file that is submitted in the same was as our `wc.jar` in our example above. for this `hadoop`-specific `jar` file, you pass executable scripts or commands as parameters to this `jar` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "note: the word \"streaming\" is used because the input and output method is unix streams (`stdin`, `stdout`), not in reference to streaming data.\n",
    "\n",
    "this is actually pretty cool, because we know how to access those streams:\n",
    "\n",
    "+ `python`: `sys` module\n",
    "+ `R`: `file(\"stdin\")` (I think? who even knows. does anyone?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when we develop a `mapper.py` script, know the following:\n",
    "\n",
    "+ *each* mapper launches the executable. spin-up time sucks for obvious reasons\n",
    "+ `hadoop streaming` parses input data into lines of text and pipes them through `stdin`\n",
    "+ `python` streaming script parses those lines of texts and prints (to `stdout`) kvps delimited in some way (default is `\\t`)\n",
    "+ these intermediate kvps are scooped up by `hadoop streaming` again and passed on to the reducer\n",
    "+ the mapper gets an entire block via `sys.stdin`. so it doesn't receive a *file*, or a *line number*, it receives a file handler to a block. that's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the `reducer.py` script follows much of the same logic, but in addition:\n",
    "\n",
    "+ the reducer doesn't receive a key and an iterable, it reads shuffled and sorted kvp records (like a table) from stdin (they are in the `a\\tb` format)\n",
    "+ a single reducer task will always get *all* records for given key, but *may* get more than one key (so your reducer doesn't have a key, we need logic there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "for both files (and for any file in any language being used as a `hadoop streaming` script), the shebang (`#!`) declaration at the top of the file is important -- it tells the streaming process (a bash shell) how to execute the script (e.g. in `python`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### real example: flight data\n",
    "\n",
    "the [bureau of transporation statistics](https://transtats.bts.gov/) makes [on-time flight data](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time) publicly available.\n",
    "\n",
    "let's download some and use a pre-written `mapper.py` and `reducer.py` to calculate the average delay per airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "first, download the airline data:\n",
    "\n",
    "```bash\n",
    "cd /tmp\n",
    "wget --no-check-certificate https://transtats.bts.gov/PREZIP/On_Time_On_Time_Performance_2017_1.zip\n",
    "unzip On_Time_On_Time_Performance_2017_1.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the code we will use is in\n",
    "\n",
    "```bash\n",
    "cd ~/code/hadoop-fundamentals/avgdelay\n",
    "```\n",
    "\n",
    "let's take a look at `mapper.py` and `reducer.py` in that repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "one nice thing about this simple framework is that we can test our functions in a simple series of pipes:\n",
    "\n",
    "```bash\n",
    "head -n100 /tmp/On_Time_On_Time_Performance_2017_1.csv | ./mapper.py | sort | ./reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so, we have seen that the `avgdelay` code is able to `map` and `reduce` the records in the `csv` of airport delays we downloaded. let's ship that over to `hdfs` and run a `streaming` job with these files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "hadoop fs -put /tmp/On_Time_On_Time_Performance_2017_1.csv .\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input On_Time_On_Time_Performance_2017_1.csv \\\n",
    "    -output average_delay \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py\n",
    "    \n",
    "# trust, but verify\n",
    "hadoop fs -cat average_delay/part* | head -n25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "note the `-files` params -- what's going on there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "in a cluster environment, the materials of the executed code will generally have to be either\n",
    "\n",
    "1. pre-installed on the cluster, so that it is obvious\n",
    "2. shipped along with the request to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "while it *is* actually still somewhat common to write `hadoop streaming` `python` scripts for doing etl work, it's really not great for data science.\n",
    "\n",
    "as you've seen, it's pretty orchestrated and low-level. we're thinking hard about the simple things we want to do (like count words or calculate averages), and when we've figured them out, we're only doing them *once*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "think about how that looks when we want to move on to something more complicated, like a gradient descent algorithm.\n",
    "\n",
    "or anything iterative, for that matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. take parameters and applying a model defined by those parameters to every record in our `hdfs` dataset -- map records of features to predicted `y` values, calcualte individual `y` error term and gradients for each record, and `emit` those\n",
    "2. reduce those partial gradients to determine the parameter update\n",
    "3. update paraemters\n",
    "\n",
    "each time we move between steps we are reading and writing to `hdfs` and that can be crazy wasteful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "also, this is 2017. we should expect that someone has just done this for us. that's fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the name of the game in data science applications is `spark`.\n",
    "\n",
    "`spark` is a `scala` (an OO functional programming language which runs on the `jvm`) application which is a fast query and iterative algorithm calculation platform. it was built as a replacement for `mapreduce` for calculation workflows just like the ones in which we are most interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "perhaps most importantly for us: there are libraries for the `spark` `api` in `python` and `R`, and this has lead to pretty wide adoption in the communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### the `spark` stack\n",
    "\n",
    "`spark` ignores data management and focuses exclusively on resource management.\n",
    "\n",
    "the primary programmer interface is the `spark` core api module (i.e. the standard library of `spark`), and this library is entirely focused on implementing commong computation tasks (file `io`, `mapping`, `reducing` , `filtering`) in as efficient a way as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "just like in `python` and `R`, after the core language has taken care of the low-level stuff, specialized tools are built on top of this. in `spark`, some of the most important are:\n",
    "\n",
    "+ `spark sql`: an implementation of the `sql` standard against `rdd`s. dirty.\n",
    "+ `spark streaming`: unbounded data stream processing\n",
    "+ `mllib` and `mahout`: common machine learning algorithms implemented\n",
    "    + `spark-sklearn` is an implementation of the ubiquitous `sklearn` `api` with `mllib` implementation under the hood\n",
    "+ `graphx`: manipulate and calculate graphs\n",
    "+ `zeppelin`: the `jupyter notebook` of the `spark` world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### resilient distributed datasets\n",
    "\n",
    "the fundamental data structure of `spark` is a resilient distributed dataset (`rdd`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "previously we cited the requirements of distributed computing frameworks to be *fault tolerance, recoverability, consistency, and scalability*\n",
    "\n",
    "`rdd`s are `spark`'s way of performing distributed computation while hitting those requirements. at the simplest level, `spark` takes a functional plan of attack (a sequence of functions) and figures out how to distribute the data to many different nodes (in memory) to optimize that plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "some important facts about `rdd`s\n",
    "\n",
    "+ `rdd`s are immutable, read-only collections of objects\n",
    "+ they can be built from a lineage (a series of `fpl` function calls)\n",
    "    + this makes them *fault tolerant, recoverable, consistent*\n",
    "+ they work in parallel, so *scalable*\n",
    "+ they are operated on by `scala`, and `fpl`, so *consistent*\n",
    "+ they are immutable, so *recoverable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when you're using the `spark` api, then, your basic abilities are to create, transform, or export these `rdd`s.\n",
    "\n",
    "you need to shift paradigms into a functional mindset: think of things you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`spark` breaks these things down into basically two types\n",
    "\n",
    "1. *transformations*: `rdd` $\\rightarrow$ new `rdd`\n",
    "    + `map`: take a big `rdd`, apply something, create a new `rdd` as a result\n",
    "2. *actions*: return something back to the client (aggregation, e.g.)\n",
    "    + `reduce`: repartition `rdd` by key, aggregate (sum, mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### programming with `rdd`s\n",
    "\n",
    "the way we actually deploy programs in `spark` is similar to how we deployed `mapreduce` jobs in `hadoop streaming`: we write some code, send it to some local machine, that distributes the computation elsewhere\n",
    "\n",
    "what changes in `spark` is that a master program (the \"driver\") creates `rdd`s by *parallelizing* a `hadoop` dataset (that is, it partitions a given dataset and pushes those partitions to nodes that perform local computations in memory).\n",
    "\n",
    "an `rdd` is a structure that manages this partitionting / parallelizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from the point of view of the `spark` program, the order of operations is\n",
    "\n",
    "1. build `rdd`s\n",
    "    + access data from `hdfs` or local disk storage\n",
    "    + parallelize that collection of data\n",
    "    + transform it as necessary\n",
    "    + cache everything we can\n",
    "2. pass *closures* (stateless functions, ignorant of the rest of the world) to each element of the `rdd`\n",
    "    + *closures* are then locally applied in-memory and the outputs are also cached\n",
    "3. output `rdd`s are *acted on* (aggregated)\n",
    "    + this is the only place we atually have an eval step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "one quick note on some common terms: *variables* and *closures*\n",
    "\n",
    "+ *closures* do not rely in any way on external data\n",
    "    + if they have variables within, they are copied to the nodes with them, but kept in local scope\n",
    "+ external data, if needed, is passed through shared variables\n",
    "    + *broadcast* variables: read only, distributed (e.g. lookup tables / stopword lists)\n",
    "    + *accumulators*: meant to be associatively updated (e.g. counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### interactive `spark` using `pyspark`\n",
    "\n",
    "`pyspark` is a `repl` for the `spark` api bindings in `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's do a quick walkthrough / demo of using `pyspark` to count the words in the `shakespeare.txt` file. you'll see that the syntax is pretty familiar from traditional `python`, with a few twists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "first, let's open the `repl`\n",
    "\n",
    "```bash\n",
    "cd ~/code/hadoop-fundamentals\n",
    "$SPARK_HOME/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and then, load the shakespeare text\n",
    "\n",
    "```python\n",
    "# sc is the \"spark context\", and it's pre-built by the repl\n",
    "text = sc.textFile('data/shakespeare.txt')\n",
    "print(text)\n",
    "help(text)\n",
    "\n",
    "# look at flatMap\n",
    "help(text.flatMap)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can use `flatMap` to apply a tokenization function to every text string:\n",
    "\n",
    "```python\n",
    "# make a tokenize function\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# looks good. let's tokenize our text into words\n",
    "words = text.flatMap(tokenize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can take that flat list of words and do our standard map and reduce. before we move on here, though, look at the linear (the defining sequence of functions) via `wc.toDebugString`\n",
    "\n",
    "```python\n",
    "# let's apply a map function for word counts\n",
    "wc = words.map(lambda x: (x, 1))\n",
    "\n",
    "# you can see the lineage:\n",
    "print(wc.toDebugString())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "finally, we'll use built-in functions to do our summation `reduce` step\n",
    "\n",
    "```python\n",
    "# include a reduce step to sort/shuffle/partition by key and add the values\n",
    "from operator import add\n",
    "cts = wc.reduceByKey(add)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "finally, save the result to file. note that this is when something actually *happens*. up until this point, we were merely defining a *lineage*; now we're actually asking that some *action* take place, and `spark` springs into action\n",
    "\n",
    "```python\n",
    "\n",
    "# finally do something with it all\n",
    "cts.saveAsTextFile('wc')\n",
    "\n",
    "# exit so we can see the results\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "# look at them beautiful results\n",
    "less wc/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `aws emr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we stressed earlier that `hadoop` is a software, not a hardware. one implementation of that software is the `aws` version of `hadoop`, which is called Elastic Map Reduce, or `emr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`emr` is both a simplification and a generalization of the sort of structure we've been talking about above.\n",
    "\n",
    "for example, the types of nodes are generalized from master / worker to include\n",
    "\n",
    "1. master (same as before)\n",
    "2. core nodes: workers which run core services and directly interface with `hdfs`\n",
    "3. task nodes: workers which do *nothing* but execute tasks (not even interface with `hdfs`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "additionally, being an `aws` product, there is immediate integration with other `aws` services, and in particular `s3`.\n",
    "\n",
    "`emr` can use `hdfs` (in `aws` land, that is ephemeral storage that is destroyed when a cluster is taken down), or it can use `s3`, or it can use the local file systems on the `aws` `ec2` servers which are actually running the cluster.\n",
    "\n",
    "generally, input and ouptut is done in `s3` and intermediate steps are retained in the ephermeral `hdfs` storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">pseudo-walkthrough: setting up an `aws emr` cluster</div>**\n",
    "\n",
    "because of the length of time involved, we will not spin up an `emr` cluster live. I have done that prior to today and have one up and running already. however, we can walk through the steps briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "the previous walkthrough showed the following\n",
    "\n",
    "1. navigate to [the `aws` `emr` service page](https://console.aws.amazon.com/elasticmapreduce/home)\n",
    "2. click the \"create cluster\" button\n",
    "    1. you could go through with this simple version, but we will go straight into \"advanced options\" (link at the top)\n",
    "    2. software and steps\n",
    "        1. leave the `emr` release at the largest value\n",
    "        2. choose your software! I will pick `hadoop`, `pig`, `hive`, `oozie`, `hue`, and `spark`\n",
    "    3. hardware\n",
    "        1. I increased the number of task servers to 3\n",
    "        2. for both core and task servers, I select spot pricing and picked a price of 0.065\n",
    "        3. the \"green\" numbers are the regions where the current spot price is lowest -- nothing more.\n",
    "            1. look at [spot pricing history](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances-history.html) for more info\n",
    "    4. I *did not* add any bootstrap actions, but if I wanted to, say, install conda everywhere, this is where I would have done it.\n",
    "3. submit and wait!\n",
    "    1. it takes a *long* time, 20+ minutes for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "at this point the most pressing matter is accessing `hue` (`h`adoop `u`ser `e`xperience).\n",
    "\n",
    "`hue` is a web application running on the master node at port 8888. it'd be great if we could just navigate there, but that port is not open to the world (and probably shouldn't be -- anyone who can access that port can run any `hadoop` command they want)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is the same setup as with any web app running on `aws` behind several firewalls.\n",
    "\n",
    "I recommend what `aws` recommends: `ssh` port forwarding. basically, we open an `ssh` pipe to the master node, and declare that whenever we go to a *local* port of a certain number (say 8888), our request is forwarded over that `ssh` connection and requests a certain port (say, also 8888) on that remote machine. the request is brougth back to us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "effectively, we are just replacing a port on our local machine with a single port on the remote machine.\n",
    "\n",
    "```bash\n",
    "ssh -i ~/.ssh/gu511_ubuntu_1.pem -N -f -L localhost:8888:localhost:8888  hadoop@ec2-54-152-19-53.compute-1.amazonaws.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "with this in place, we should be able to go to http://localhost:8888/ and start kicking around our new cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">`hue`-based `emr`-walkthrough</div>**\n",
    "\n",
    "+ let's put some data out there. go to the `hue` file browser\n",
    "    + textcorpus\n",
    "        + locally: `wget https://github.com/bbengfort/hadoop-fundamentals/blob/master/data/textcorpus.zip?raw=true`\n",
    "        + rename it, upload that via \"Upload > zip\"\n",
    "    + shakespeare\n",
    "        + local: `wget https://github.com/bbengfort/hadoop-fundamentals/blob/master/data/shakespeare.txt.zip?raw=true`\n",
    "        + rename it, upload it\n",
    "    + python data\n",
    "        + download https://s3.amazonaws.com/hadoop.rzl.gu511.com/pyq_stripped.tar.gz\n",
    "        + possibly load it in directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ hive (via hue)\n",
    "    + create new database `stack_overflow_python`\n",
    "    + point to the `s3` upload in `hadoop.rzl.gu511.com`\n",
    "    + did three simple queries, all saved and available for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### software lightning round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `hadoop` and `spark`\n",
    "\n",
    "the current leaders in the distributed data and distributed computation sphere; we've talked about them enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `pig`\n",
    "\n",
    "[`pig`](https://pig.apache.org/) is a declarative querying / scripting language (not like `sql`, more like `spark`). bascially this is a higher-order abstraction of `mapreduce` functions that can be extremely fast while also being much easier to read and write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `hive`\n",
    "\n",
    "[`hive`](https://hive.apache.org/) is the `sql` of the `hadoop` ecosystem. it also doubles as a data warehousing platform for many of the other applications. it implements batch querying (not interactive querying), and is not *itself* a database, just a means of interacting with other data structures\n",
    "\n",
    "I personally find that `hive` is the easiest of the `hadoop` ecosystem tools to spin up in (especially if you have a background in `sql`), so you may want to consider staring here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `hcatalog`\n",
    "[`hcatalog`](https://cwiki.apache.org/confluence/display/Hive/HCatalog) is a sub-component of `hive` that sees wide use in other applications. it is the table and storage management layer, and is used by disparate apps to provide a normalized, gridded view of data of many different formats. it acts as a normalized layer between application and data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `oozie`\n",
    "\n",
    "[`oozie`](http://oozie.apache.org/) is the scheduling and workflow management tool for `hadoop` jobs. it uses `xml` configuration files to define chained DAGs of jobs, executes them, reports on results and logs, handles concurrency and timing issues, and provides for a complex set of control flows.\n",
    "\n",
    "personally I think it's pretty janky, but its' the best you've got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `hue`\n",
    "\n",
    "[`hue`](http://gethue.com/) (short for `h`adoop `u`ser `e`xperience) aims to be the way that users experience `hadoop`, and that's a great thing -- almost everything you might want to do in the `hadoop` ecosystem will be easier (at first) to do in `hue`.\n",
    "\n",
    "over time, of course, complications and bugs will push you down (up?) into the command line and the programs themselves, but for beginning it is *essential* that you try using `hue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mahout`\n",
    "\n",
    "[`mahout`](http://mahout.apache.org/) is a framework for building machine learning applications. it is not immediately clear to me the extent to which `mahout` and `spark`'s `mllib` are *competitors* or *complimentors*, and I haven't been able to clear that up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `sqoop`\n",
    "\n",
    "[`sqoop`](http://sqoop.apache.org/) (`sq`l `o`n hado`op`) is a bulk data transfer took, looking to implement (under the hood) `mapreduce` jobs to perform data transfer between traditional databases, `hdfs`, and other data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `tez`\n",
    "[`tez`](https://tez.apache.org/) is similar to `spark` in that it is positioning itself as a replacement for `mapreduce` for simpler application frameworks. however, it is trying to be much closer to the `hadoop` / `yarn` infrastructure, implementing a normalized `api` so that other tools (the `pig`s and `hive`s of the world) can run directly off of `tez` in a normalized way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `hbase`\n",
    "[`hbase`](https://hbase.apache.org/) is a big data database and date store. it is actually a `nosql` key-value store on `hdfs`, and it has its own query language (like many `nosql` dbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `phoenix`\n",
    "[`phoenix`](https://phoenix.apache.org/) \n",
    "\n",
    "an on-line transaction processing software for performing data transactions in a `sql` syntax, but using `hbase` as its backing (so schemaless)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `presto`\n",
    "[`presto`](https://prestodb.io/) is a `sql` query engine for interactive queries against big data platforms. it was developed at facebook so it has good integration with `cassandra` in addition to other data stores.\n",
    "\n",
    "it's main goal is to be interactive and very fast. I haven't used it myself but the reviews are pretty positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `zeppelin`\n",
    "[`zeppelin`](https://zeppelin.apache.org/) is the `jupyter notebook` of the `spark` world, allowing for collaborative and exploratory work to be done in a web-based notebook. current interpreters include `spark`, `sql`, `python`, `hive`, `pig`, `sparksql`, `markdown`, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `flink`\n",
    "[`flink`](https://flink.apache.org) is a stream processing framework. many of the big data sources of import are streams of data (log files, e.g.) and `flink` aims to be the standard means of persisting data streams into distributed environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `zookeeper`\n",
    "[`zookeeper`](https://zookeeper.apache.org/) is a centralized configuration engine. with so many moving parts in the distributed cluster environment, this can be pretty essential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `livy`\n",
    "[`livy`](https://livy.incubator.apache.org/) is a `REST` interface for `spark`, allowing users to submit `spark` jobs from anywhere (not just on the cluster, in a `repl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `ganglia`\n",
    "[`ganglia`](http://ganglia.sourceforge.net/) is the primary distributed monitoring system for `hadoop` clusters and grid computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mxnet`\n",
    "[`mxnet`](https://mxnet.incubator.apache.org/) is an apache incubator project for developing distributed deep learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***<div align=\"center\">`hadoop` is `hadope`</div>***\n",
    "<div align=\"center\"><img src=\"https://pbs.twimg.com/media/Bnz0UglCUAA_Kb-.jpg\"></div>\n",
    "\n",
    "# END OF LECTURE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
