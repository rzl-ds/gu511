{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## intro\n",
    "\n",
    "it's hard to understate the pervasiveness and success of deep learning methods in recent years. knowledge of deep learning techniques is a must for modern data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "it's so important, in fact, that GU offers an entire class on it: [Math 514: intro to neural networks](https://myaccess.georgetown.edu/pls/bninbp/bwckctlg.p_display_courses?term_in=201910&one_subj=MATH&sel_crse_strt=514&sel_crse_end=514&sel_subj=&sel_levl=&sel_schd=&sel_coll=&sel_divs=&sel_dept=&sel_attr=#_ga=2.146187319.2080322458.1542654672-115011657.1531320772). this is not that course! you should consider taking it.\n",
    "\n",
    "what follows is merely an incredibly hand-waivy introduction to deep neural nets. I hope to give you enough understanding and context that you feel comfortable executing simple deep learning code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### deep learning vs. deep neural nets\n",
    "\n",
    "for starters, a bit of nomenclature: the lecture is called \"deep learning\" but I will often be talking about \"deep neural nets\" instead. they are related:\n",
    "\n",
    "+ **deep learning** is a family of statistical modelling approaches that attempt to \"learn\" the underlying structure or most convenient representation of data in order to make a specific sort of prediction\n",
    "    + \"learning\" happens through exposure to subsequent examples. a model trained as is should become a better model if exposed to a new example\n",
    "    + the predictions made in deep learning are typically supervised (real-world targets), but not necessarily so (autoencoders)\n",
    "+ **deep neural nets** are a sub-family of *deep learning* models that are specifically constructed out of inter-connected \"neurons\", computation steps that perform a linear transformation and then a subsequent nonlinear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "it's a minor distinction, but there are things that are **deep learning** that are not **deep neural nets**; we're not going to talk about them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### introduction to deep neural nets\n",
    "\n",
    "let's talk about what a deep neural net is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### one neuron / node\n",
    "the fundamental element of a neural net is the neuron. this is so named due to long-standing analogies to the way neurons work in a brain.\n",
    "\n",
    "I think this analogy is more confusing than it is worth. *just watch me* call them nodes instead of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the ~~neuron~~ node is a two-step operation: you do one *linear* transformation with a vector of weights and a bias value, then you do one *nonlinear* transformation with some function (called the **activation function**).\n",
    "\n",
    "what weights? what bias value? what function? to be determined!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "suppose we have a record of data with two features $x_1$ and $x_2$. a neuron that can act on that record would have two weight values ($w_1$ and $w_2$, one for each feature), a bias value $b$, and an activation function $f$\n",
    "\n",
    "<br><div align=\"center\"><img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=568&h=303\" width=\"600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the first step is the *linear* transformation. symoblically, this is:\n",
    "\n",
    "$$\n",
    "W \\cdot x + b = \\sum_i W_i x_i + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "geometrically, this is a measurement of how large the vector $x$ is when projected along the weight $W$\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1DYpFUfRxuuAhn66402TJOMbyXCzf4Zcw\" width=\"600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "basically, we are *linearizing* the input by converting every incoming record in whatever space to one single number measuring the amount of that vector pointing in some specific direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "after we have linearized the input, we add a non-linearity using an **activation function**. this activation function takes one input value (the linearized input value) and outputs something that is specifically non-linear.\n",
    "\n",
    "there are [a lot of these functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions), but the most common are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the **sigmoid**, $\\sigma(x) = \\dfrac{1}{1 + \\exp(-x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "7352348b-905b-4a36-b7eb-beca82cafcac",
         "x": [
          -10,
          -9.797979797979798,
          -9.595959595959595,
          -9.393939393939394,
          -9.191919191919192,
          -8.98989898989899,
          -8.787878787878787,
          -8.585858585858587,
          -8.383838383838384,
          -8.181818181818182,
          -7.979797979797979,
          -7.777777777777778,
          -7.575757575757576,
          -7.373737373737374,
          -7.171717171717171,
          -6.96969696969697,
          -6.767676767676768,
          -6.565656565656566,
          -6.363636363636363,
          -6.161616161616162,
          -5.959595959595959,
          -5.757575757575758,
          -5.555555555555555,
          -5.353535353535354,
          -5.151515151515151,
          -4.94949494949495,
          -4.747474747474747,
          -4.545454545454546,
          -4.343434343434343,
          -4.141414141414142,
          -3.9393939393939394,
          -3.737373737373738,
          -3.5353535353535355,
          -3.333333333333333,
          -3.1313131313131315,
          -2.929292929292929,
          -2.7272727272727275,
          -2.525252525252525,
          -2.3232323232323235,
          -2.121212121212121,
          -1.9191919191919187,
          -1.717171717171718,
          -1.5151515151515156,
          -1.3131313131313131,
          -1.1111111111111107,
          -0.9090909090909101,
          -0.7070707070707076,
          -0.5050505050505052,
          -0.30303030303030276,
          -0.10101010101010033,
          0.10101010101010033,
          0.30303030303030276,
          0.5050505050505052,
          0.7070707070707076,
          0.9090909090909083,
          1.1111111111111107,
          1.3131313131313131,
          1.5151515151515156,
          1.7171717171717162,
          1.9191919191919187,
          2.121212121212121,
          2.3232323232323235,
          2.525252525252524,
          2.7272727272727266,
          2.929292929292929,
          3.1313131313131315,
          3.333333333333334,
          3.5353535353535346,
          3.737373737373737,
          3.9393939393939394,
          4.141414141414142,
          4.3434343434343425,
          4.545454545454545,
          4.747474747474747,
          4.94949494949495,
          5.1515151515151505,
          5.353535353535353,
          5.555555555555555,
          5.757575757575758,
          5.9595959595959584,
          6.161616161616163,
          6.363636363636363,
          6.565656565656564,
          6.767676767676768,
          6.969696969696969,
          7.171717171717173,
          7.373737373737374,
          7.575757575757574,
          7.777777777777779,
          7.979797979797979,
          8.18181818181818,
          8.383838383838384,
          8.585858585858585,
          8.787878787878789,
          8.98989898989899,
          9.19191919191919,
          9.393939393939394,
          9.595959595959595,
          9.7979797979798,
          10
         ],
         "y": [
          4.5397868702434395e-05,
          5.556064893935847e-05,
          6.7998317442358e-05,
          8.322001972209245e-05,
          0.00010184881542721271,
          0.00012464714594414533,
          0.00015254798624649647,
          0.00018669294496130814,
          0.00022847885532128418,
          0.0002796147386491923,
          0.00034219143371662803,
          0.0004187666844443735,
          0.0005124690821944584,
          0.0006271249872756847,
          0.000767413429918266,
          0.0009390550390618305,
          0.001149042294880872,
          0.0014059198755086322,
          0.0017201255952192596,
          0.0021044044291184333,
          0.0025743103931375314,
          0.0031488135776798513,
          0.003851032355930255,
          0.004709113572114011,
          0.0057572861219081835,
          0.007037115364564376,
          0.008598986610189428,
          0.010503844513285416,
          0.012825210092764685,
          0.015651486142818367,
          0.019088541989923387,
          0.0232625358308855,
          0.0283228820443032,
          0.03444519566621118,
          0.04183394000551971,
          0.05072436056205437,
          0.06138310740349217,
          0.07410673632504733,
          0.08921706025119844,
          0.10705214621417715,
          0.12795170492445268,
          0.15223582314389053,
          0.18017659335766972,
          0.21196333386923713,
          0.2476638011390717,
          0.28718590138250244,
          0.33024642963181144,
          0.37635451749670706,
          0.42481686806284763,
          0.4747689239079115,
          0.5252310760920885,
          0.5751831319371523,
          0.623645482503293,
          0.6697535703681886,
          0.7128140986174972,
          0.7523361988609284,
          0.7880366661307628,
          0.8198234066423302,
          0.8477641768561092,
          0.8720482950755473,
          0.8929478537858229,
          0.9107829397488015,
          0.9258932636749527,
          0.9386168925965077,
          0.9492756394379457,
          0.9581660599944802,
          0.9655548043337889,
          0.9716771179556968,
          0.9767374641691144,
          0.9809114580100765,
          0.9843485138571816,
          0.9871747899072354,
          0.9894961554867145,
          0.9914010133898106,
          0.9929628846354357,
          0.9942427138780918,
          0.995290886427886,
          0.9961489676440697,
          0.9968511864223202,
          0.9974256896068625,
          0.9978955955708816,
          0.9982798744047808,
          0.9985940801244912,
          0.9988509577051191,
          0.9990609449609381,
          0.9992325865700818,
          0.9993728750127243,
          0.9994875309178055,
          0.9995812333155556,
          0.9996578085662833,
          0.9997203852613508,
          0.9997715211446788,
          0.9998133070550387,
          0.9998474520137535,
          0.9998753528540559,
          0.9998981511845727,
          0.999916779980278,
          0.9999320016825577,
          0.9999444393510606,
          0.9999546021312976
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"64450f65-9704-4edd-b623-3b730f43c3ab\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"64450f65-9704-4edd-b623-3b730f43c3ab\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [4.5397868702434395e-05, 5.556064893935847e-05, 6.7998317442358e-05, 8.322001972209245e-05, 0.00010184881542721271, 0.00012464714594414533, 0.00015254798624649647, 0.00018669294496130814, 0.00022847885532128418, 0.0002796147386491923, 0.00034219143371662803, 0.0004187666844443735, 0.0005124690821944584, 0.0006271249872756847, 0.000767413429918266, 0.0009390550390618305, 0.001149042294880872, 0.0014059198755086322, 0.0017201255952192596, 0.0021044044291184333, 0.0025743103931375314, 0.0031488135776798513, 0.003851032355930255, 0.004709113572114011, 0.0057572861219081835, 0.007037115364564376, 0.008598986610189428, 0.010503844513285416, 0.012825210092764685, 0.015651486142818367, 0.019088541989923387, 0.0232625358308855, 0.0283228820443032, 0.03444519566621118, 0.04183394000551971, 0.05072436056205437, 0.06138310740349217, 0.07410673632504733, 0.08921706025119844, 0.10705214621417715, 0.12795170492445268, 0.15223582314389053, 0.18017659335766972, 0.21196333386923713, 0.2476638011390717, 0.28718590138250244, 0.33024642963181144, 0.37635451749670706, 0.42481686806284763, 0.4747689239079115, 0.5252310760920885, 0.5751831319371523, 0.623645482503293, 0.6697535703681886, 0.7128140986174972, 0.7523361988609284, 0.7880366661307628, 0.8198234066423302, 0.8477641768561092, 0.8720482950755473, 0.8929478537858229, 0.9107829397488015, 0.9258932636749527, 0.9386168925965077, 0.9492756394379457, 0.9581660599944802, 0.9655548043337889, 0.9716771179556968, 0.9767374641691144, 0.9809114580100765, 0.9843485138571816, 0.9871747899072354, 0.9894961554867145, 0.9914010133898106, 0.9929628846354357, 0.9942427138780918, 0.995290886427886, 0.9961489676440697, 0.9968511864223202, 0.9974256896068625, 0.9978955955708816, 0.9982798744047808, 0.9985940801244912, 0.9988509577051191, 0.9990609449609381, 0.9992325865700818, 0.9993728750127243, 0.9994875309178055, 0.9995812333155556, 0.9996578085662833, 0.9997203852613508, 0.9997715211446788, 0.9998133070550387, 0.9998474520137535, 0.9998753528540559, 0.9998981511845727, 0.999916779980278, 0.9999320016825577, 0.9999444393510606, 0.9999546021312976], \"type\": \"scatter\", \"uid\": \"a23d5643-b55a-4c55-b767-13ebc8a89f77\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"64450f65-9704-4edd-b623-3b730f43c3ab\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"64450f65-9704-4edd-b623-3b730f43c3ab\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"64450f65-9704-4edd-b623-3b730f43c3ab\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [4.5397868702434395e-05, 5.556064893935847e-05, 6.7998317442358e-05, 8.322001972209245e-05, 0.00010184881542721271, 0.00012464714594414533, 0.00015254798624649647, 0.00018669294496130814, 0.00022847885532128418, 0.0002796147386491923, 0.00034219143371662803, 0.0004187666844443735, 0.0005124690821944584, 0.0006271249872756847, 0.000767413429918266, 0.0009390550390618305, 0.001149042294880872, 0.0014059198755086322, 0.0017201255952192596, 0.0021044044291184333, 0.0025743103931375314, 0.0031488135776798513, 0.003851032355930255, 0.004709113572114011, 0.0057572861219081835, 0.007037115364564376, 0.008598986610189428, 0.010503844513285416, 0.012825210092764685, 0.015651486142818367, 0.019088541989923387, 0.0232625358308855, 0.0283228820443032, 0.03444519566621118, 0.04183394000551971, 0.05072436056205437, 0.06138310740349217, 0.07410673632504733, 0.08921706025119844, 0.10705214621417715, 0.12795170492445268, 0.15223582314389053, 0.18017659335766972, 0.21196333386923713, 0.2476638011390717, 0.28718590138250244, 0.33024642963181144, 0.37635451749670706, 0.42481686806284763, 0.4747689239079115, 0.5252310760920885, 0.5751831319371523, 0.623645482503293, 0.6697535703681886, 0.7128140986174972, 0.7523361988609284, 0.7880366661307628, 0.8198234066423302, 0.8477641768561092, 0.8720482950755473, 0.8929478537858229, 0.9107829397488015, 0.9258932636749527, 0.9386168925965077, 0.9492756394379457, 0.9581660599944802, 0.9655548043337889, 0.9716771179556968, 0.9767374641691144, 0.9809114580100765, 0.9843485138571816, 0.9871747899072354, 0.9894961554867145, 0.9914010133898106, 0.9929628846354357, 0.9942427138780918, 0.995290886427886, 0.9961489676440697, 0.9968511864223202, 0.9974256896068625, 0.9978955955708816, 0.9982798744047808, 0.9985940801244912, 0.9988509577051191, 0.9990609449609381, 0.9992325865700818, 0.9993728750127243, 0.9994875309178055, 0.9995812333155556, 0.9996578085662833, 0.9997203852613508, 0.9997715211446788, 0.9998133070550387, 0.9998474520137535, 0.9998753528540559, 0.9998981511845727, 0.999916779980278, 0.9999320016825577, 0.9999444393510606, 0.9999546021312976], \"type\": \"scatter\", \"uid\": \"a23d5643-b55a-4c55-b767-13ebc8a89f77\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"64450f65-9704-4edd-b623-3b730f43c3ab\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline, plotly.graph_objs as go; plotly.offline.init_notebook_mode(connected=True)\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "data = [go.Scatter(x=x, y=y)]\n",
    "plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the **ReLU** (**Re**ctified **L**inear **U**nit),\n",
    "\n",
    "$$\n",
    "\\operatorname{relu}(x) = \\left\\{\\begin{array}{ll}\n",
    "0 & x \\leq 0 \\\\\n",
    "x & x > 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "439fad50-49db-432a-a624-446de5b7aacd",
         "x": [
          -10,
          -9.797979797979798,
          -9.595959595959595,
          -9.393939393939394,
          -9.191919191919192,
          -8.98989898989899,
          -8.787878787878787,
          -8.585858585858587,
          -8.383838383838384,
          -8.181818181818182,
          -7.979797979797979,
          -7.777777777777778,
          -7.575757575757576,
          -7.373737373737374,
          -7.171717171717171,
          -6.96969696969697,
          -6.767676767676768,
          -6.565656565656566,
          -6.363636363636363,
          -6.161616161616162,
          -5.959595959595959,
          -5.757575757575758,
          -5.555555555555555,
          -5.353535353535354,
          -5.151515151515151,
          -4.94949494949495,
          -4.747474747474747,
          -4.545454545454546,
          -4.343434343434343,
          -4.141414141414142,
          -3.9393939393939394,
          -3.737373737373738,
          -3.5353535353535355,
          -3.333333333333333,
          -3.1313131313131315,
          -2.929292929292929,
          -2.7272727272727275,
          -2.525252525252525,
          -2.3232323232323235,
          -2.121212121212121,
          -1.9191919191919187,
          -1.717171717171718,
          -1.5151515151515156,
          -1.3131313131313131,
          -1.1111111111111107,
          -0.9090909090909101,
          -0.7070707070707076,
          -0.5050505050505052,
          -0.30303030303030276,
          -0.10101010101010033,
          0.10101010101010033,
          0.30303030303030276,
          0.5050505050505052,
          0.7070707070707076,
          0.9090909090909083,
          1.1111111111111107,
          1.3131313131313131,
          1.5151515151515156,
          1.7171717171717162,
          1.9191919191919187,
          2.121212121212121,
          2.3232323232323235,
          2.525252525252524,
          2.7272727272727266,
          2.929292929292929,
          3.1313131313131315,
          3.333333333333334,
          3.5353535353535346,
          3.737373737373737,
          3.9393939393939394,
          4.141414141414142,
          4.3434343434343425,
          4.545454545454545,
          4.747474747474747,
          4.94949494949495,
          5.1515151515151505,
          5.353535353535353,
          5.555555555555555,
          5.757575757575758,
          5.9595959595959584,
          6.161616161616163,
          6.363636363636363,
          6.565656565656564,
          6.767676767676768,
          6.969696969696969,
          7.171717171717173,
          7.373737373737374,
          7.575757575757574,
          7.777777777777779,
          7.979797979797979,
          8.18181818181818,
          8.383838383838384,
          8.585858585858585,
          8.787878787878789,
          8.98989898989899,
          9.19191919191919,
          9.393939393939394,
          9.595959595959595,
          9.7979797979798,
          10
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.10101010101010033,
          0.30303030303030276,
          0.5050505050505052,
          0.7070707070707076,
          0.9090909090909083,
          1.1111111111111107,
          1.3131313131313131,
          1.5151515151515156,
          1.7171717171717162,
          1.9191919191919187,
          2.121212121212121,
          2.3232323232323235,
          2.525252525252524,
          2.7272727272727266,
          2.929292929292929,
          3.1313131313131315,
          3.333333333333334,
          3.5353535353535346,
          3.737373737373737,
          3.9393939393939394,
          4.141414141414142,
          4.3434343434343425,
          4.545454545454545,
          4.747474747474747,
          4.94949494949495,
          5.1515151515151505,
          5.353535353535353,
          5.555555555555555,
          5.757575757575758,
          5.9595959595959584,
          6.161616161616163,
          6.363636363636363,
          6.565656565656564,
          6.767676767676768,
          6.969696969696969,
          7.171717171717173,
          7.373737373737374,
          7.575757575757574,
          7.777777777777779,
          7.979797979797979,
          8.18181818181818,
          8.383838383838384,
          8.585858585858585,
          8.787878787878789,
          8.98989898989899,
          9.19191919191919,
          9.393939393939394,
          9.595959595959595,
          9.7979797979798,
          10
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"9fa55eee-cb75-46c6-a801-6e0c070c0901\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9fa55eee-cb75-46c6-a801-6e0c070c0901\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"type\": \"scatter\", \"uid\": \"f5bd9545-4c7f-42f5-adb5-6006b4258293\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"9fa55eee-cb75-46c6-a801-6e0c070c0901\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9fa55eee-cb75-46c6-a801-6e0c070c0901\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9fa55eee-cb75-46c6-a801-6e0c070c0901\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"type\": \"scatter\", \"uid\": \"f5bd9545-4c7f-42f5-adb5-6006b4258293\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"9fa55eee-cb75-46c6-a801-6e0c070c0901\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.where(x <= 0, 0, x)\n",
    "data = [go.Scatter(x=x, y=y)]\n",
    "plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the **leaky ReLU**,\n",
    "\n",
    "$$\n",
    "\\operatorname{relu}(x) = \\left\\{\\begin{array}{ll}\n",
    "0.01 x & x \\leq 0 \\\\\n",
    "x & x > 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "3dd187cb-7601-4478-871c-ae52b9d51df7",
         "x": [
          -10,
          -9.797979797979798,
          -9.595959595959595,
          -9.393939393939394,
          -9.191919191919192,
          -8.98989898989899,
          -8.787878787878787,
          -8.585858585858587,
          -8.383838383838384,
          -8.181818181818182,
          -7.979797979797979,
          -7.777777777777778,
          -7.575757575757576,
          -7.373737373737374,
          -7.171717171717171,
          -6.96969696969697,
          -6.767676767676768,
          -6.565656565656566,
          -6.363636363636363,
          -6.161616161616162,
          -5.959595959595959,
          -5.757575757575758,
          -5.555555555555555,
          -5.353535353535354,
          -5.151515151515151,
          -4.94949494949495,
          -4.747474747474747,
          -4.545454545454546,
          -4.343434343434343,
          -4.141414141414142,
          -3.9393939393939394,
          -3.737373737373738,
          -3.5353535353535355,
          -3.333333333333333,
          -3.1313131313131315,
          -2.929292929292929,
          -2.7272727272727275,
          -2.525252525252525,
          -2.3232323232323235,
          -2.121212121212121,
          -1.9191919191919187,
          -1.717171717171718,
          -1.5151515151515156,
          -1.3131313131313131,
          -1.1111111111111107,
          -0.9090909090909101,
          -0.7070707070707076,
          -0.5050505050505052,
          -0.30303030303030276,
          -0.10101010101010033,
          0.10101010101010033,
          0.30303030303030276,
          0.5050505050505052,
          0.7070707070707076,
          0.9090909090909083,
          1.1111111111111107,
          1.3131313131313131,
          1.5151515151515156,
          1.7171717171717162,
          1.9191919191919187,
          2.121212121212121,
          2.3232323232323235,
          2.525252525252524,
          2.7272727272727266,
          2.929292929292929,
          3.1313131313131315,
          3.333333333333334,
          3.5353535353535346,
          3.737373737373737,
          3.9393939393939394,
          4.141414141414142,
          4.3434343434343425,
          4.545454545454545,
          4.747474747474747,
          4.94949494949495,
          5.1515151515151505,
          5.353535353535353,
          5.555555555555555,
          5.757575757575758,
          5.9595959595959584,
          6.161616161616163,
          6.363636363636363,
          6.565656565656564,
          6.767676767676768,
          6.969696969696969,
          7.171717171717173,
          7.373737373737374,
          7.575757575757574,
          7.777777777777779,
          7.979797979797979,
          8.18181818181818,
          8.383838383838384,
          8.585858585858585,
          8.787878787878789,
          8.98989898989899,
          9.19191919191919,
          9.393939393939394,
          9.595959595959595,
          9.7979797979798,
          10
         ],
         "y": [
          -0.1,
          -0.09797979797979797,
          -0.09595959595959595,
          -0.09393939393939395,
          -0.09191919191919193,
          -0.0898989898989899,
          -0.08787878787878788,
          -0.08585858585858587,
          -0.08383838383838384,
          -0.08181818181818182,
          -0.0797979797979798,
          -0.07777777777777778,
          -0.07575757575757576,
          -0.07373737373737374,
          -0.07171717171717172,
          -0.0696969696969697,
          -0.06767676767676768,
          -0.06565656565656566,
          -0.06363636363636363,
          -0.06161616161616162,
          -0.05959595959595959,
          -0.05757575757575758,
          -0.05555555555555555,
          -0.05353535353535354,
          -0.051515151515151514,
          -0.0494949494949495,
          -0.047474747474747475,
          -0.04545454545454546,
          -0.043434343434343436,
          -0.04141414141414142,
          -0.0393939393939394,
          -0.03737373737373738,
          -0.03535353535353535,
          -0.03333333333333333,
          -0.031313131313131314,
          -0.02929292929292929,
          -0.027272727272727275,
          -0.025252525252525252,
          -0.023232323232323236,
          -0.02121212121212121,
          -0.019191919191919187,
          -0.01717171717171718,
          -0.015151515151515155,
          -0.013131313131313133,
          -0.011111111111111108,
          -0.009090909090909101,
          -0.007070707070707076,
          -0.005050505050505052,
          -0.0030303030303030277,
          -0.0010101010101010034,
          0.10101010101010033,
          0.30303030303030276,
          0.5050505050505052,
          0.7070707070707076,
          0.9090909090909083,
          1.1111111111111107,
          1.3131313131313131,
          1.5151515151515156,
          1.7171717171717162,
          1.9191919191919187,
          2.121212121212121,
          2.3232323232323235,
          2.525252525252524,
          2.7272727272727266,
          2.929292929292929,
          3.1313131313131315,
          3.333333333333334,
          3.5353535353535346,
          3.737373737373737,
          3.9393939393939394,
          4.141414141414142,
          4.3434343434343425,
          4.545454545454545,
          4.747474747474747,
          4.94949494949495,
          5.1515151515151505,
          5.353535353535353,
          5.555555555555555,
          5.757575757575758,
          5.9595959595959584,
          6.161616161616163,
          6.363636363636363,
          6.565656565656564,
          6.767676767676768,
          6.969696969696969,
          7.171717171717173,
          7.373737373737374,
          7.575757575757574,
          7.777777777777779,
          7.979797979797979,
          8.18181818181818,
          8.383838383838384,
          8.585858585858585,
          8.787878787878789,
          8.98989898989899,
          9.19191919191919,
          9.393939393939394,
          9.595959595959595,
          9.7979797979798,
          10
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"3a0cb38d-596d-4dcf-961f-068ae62544ab\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3a0cb38d-596d-4dcf-961f-068ae62544ab\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [-0.1, -0.09797979797979797, -0.09595959595959595, -0.09393939393939395, -0.09191919191919193, -0.0898989898989899, -0.08787878787878788, -0.08585858585858587, -0.08383838383838384, -0.08181818181818182, -0.0797979797979798, -0.07777777777777778, -0.07575757575757576, -0.07373737373737374, -0.07171717171717172, -0.0696969696969697, -0.06767676767676768, -0.06565656565656566, -0.06363636363636363, -0.06161616161616162, -0.05959595959595959, -0.05757575757575758, -0.05555555555555555, -0.05353535353535354, -0.051515151515151514, -0.0494949494949495, -0.047474747474747475, -0.04545454545454546, -0.043434343434343436, -0.04141414141414142, -0.0393939393939394, -0.03737373737373738, -0.03535353535353535, -0.03333333333333333, -0.031313131313131314, -0.02929292929292929, -0.027272727272727275, -0.025252525252525252, -0.023232323232323236, -0.02121212121212121, -0.019191919191919187, -0.01717171717171718, -0.015151515151515155, -0.013131313131313133, -0.011111111111111108, -0.009090909090909101, -0.007070707070707076, -0.005050505050505052, -0.0030303030303030277, -0.0010101010101010034, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"type\": \"scatter\", \"uid\": \"42179a67-423b-4fc9-beec-1a119d0cf498\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"3a0cb38d-596d-4dcf-961f-068ae62544ab\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"3a0cb38d-596d-4dcf-961f-068ae62544ab\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3a0cb38d-596d-4dcf-961f-068ae62544ab\", [{\"x\": [-10.0, -9.797979797979798, -9.595959595959595, -9.393939393939394, -9.191919191919192, -8.98989898989899, -8.787878787878787, -8.585858585858587, -8.383838383838384, -8.181818181818182, -7.979797979797979, -7.777777777777778, -7.575757575757576, -7.373737373737374, -7.171717171717171, -6.96969696969697, -6.767676767676768, -6.565656565656566, -6.363636363636363, -6.161616161616162, -5.959595959595959, -5.757575757575758, -5.555555555555555, -5.353535353535354, -5.151515151515151, -4.94949494949495, -4.747474747474747, -4.545454545454546, -4.343434343434343, -4.141414141414142, -3.9393939393939394, -3.737373737373738, -3.5353535353535355, -3.333333333333333, -3.1313131313131315, -2.929292929292929, -2.7272727272727275, -2.525252525252525, -2.3232323232323235, -2.121212121212121, -1.9191919191919187, -1.717171717171718, -1.5151515151515156, -1.3131313131313131, -1.1111111111111107, -0.9090909090909101, -0.7070707070707076, -0.5050505050505052, -0.30303030303030276, -0.10101010101010033, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"y\": [-0.1, -0.09797979797979797, -0.09595959595959595, -0.09393939393939395, -0.09191919191919193, -0.0898989898989899, -0.08787878787878788, -0.08585858585858587, -0.08383838383838384, -0.08181818181818182, -0.0797979797979798, -0.07777777777777778, -0.07575757575757576, -0.07373737373737374, -0.07171717171717172, -0.0696969696969697, -0.06767676767676768, -0.06565656565656566, -0.06363636363636363, -0.06161616161616162, -0.05959595959595959, -0.05757575757575758, -0.05555555555555555, -0.05353535353535354, -0.051515151515151514, -0.0494949494949495, -0.047474747474747475, -0.04545454545454546, -0.043434343434343436, -0.04141414141414142, -0.0393939393939394, -0.03737373737373738, -0.03535353535353535, -0.03333333333333333, -0.031313131313131314, -0.02929292929292929, -0.027272727272727275, -0.025252525252525252, -0.023232323232323236, -0.02121212121212121, -0.019191919191919187, -0.01717171717171718, -0.015151515151515155, -0.013131313131313133, -0.011111111111111108, -0.009090909090909101, -0.007070707070707076, -0.005050505050505052, -0.0030303030303030277, -0.0010101010101010034, 0.10101010101010033, 0.30303030303030276, 0.5050505050505052, 0.7070707070707076, 0.9090909090909083, 1.1111111111111107, 1.3131313131313131, 1.5151515151515156, 1.7171717171717162, 1.9191919191919187, 2.121212121212121, 2.3232323232323235, 2.525252525252524, 2.7272727272727266, 2.929292929292929, 3.1313131313131315, 3.333333333333334, 3.5353535353535346, 3.737373737373737, 3.9393939393939394, 4.141414141414142, 4.3434343434343425, 4.545454545454545, 4.747474747474747, 4.94949494949495, 5.1515151515151505, 5.353535353535353, 5.555555555555555, 5.757575757575758, 5.9595959595959584, 6.161616161616163, 6.363636363636363, 6.565656565656564, 6.767676767676768, 6.969696969696969, 7.171717171717173, 7.373737373737374, 7.575757575757574, 7.777777777777779, 7.979797979797979, 8.18181818181818, 8.383838383838384, 8.585858585858585, 8.787878787878789, 8.98989898989899, 9.19191919191919, 9.393939393939394, 9.595959595959595, 9.7979797979798, 10.0], \"type\": \"scatter\", \"uid\": \"42179a67-423b-4fc9-beec-1a119d0cf498\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"3a0cb38d-596d-4dcf-961f-068ae62544ab\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.where(x <= 0, 0.01 * x, x)\n",
    "data = [go.Scatter(x=x, y=y)]\n",
    "plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a stack of neurons\n",
    "\n",
    "once we understand what one neuron is doing, we could take a whole stack of $N$ of them. each could have different $W$ and $b$ values. they could have different activation functions (but typically don't). and they all could take one input record and create $N$ output values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is often visualized as a \"net\", where the inputs and neurons (nodes) are drawn as circles, and the \"weights\" are represented as edges (that is, edge from a node to $x_i$ represents that nodes' $w_i$ value)\n",
    "\n",
    "<br><img src=\"https://draftin.com/images/34466?token=YFsmpDuQfD3DDylinRD8F4sLOgjCFm4Aow1gIWoCY5KED3bnQKs17RaTja95OIQQWdr25dqS2fxq_6mDwwdcs9Y\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so with a stack of $N$ nodes we can convert an input record $x$ into an $N$-dimensional output record $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a stack of stack of neurons\n",
    "\n",
    "the output of one stack of neurons is a new record. it's in some crazy $N$-dimensional space which is determined by the weights and biases of the previous layer, but it's basically now just a new record.\n",
    "\n",
    "so we could do the same thing with *that* record that we did with our $x$ records, and feed it into a *new* stack of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is the \"deep\" neural net -- it's a neural net with hidden layers, so it's become \"deep\"\n",
    "\n",
    "<br><img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### finally, an output\n",
    "\n",
    "remember, we started down this path because our model we are constructing should be able to *predict* something. so we need a final layer that will take... whatever it is that we've created -- whatever that representation is -- and predict a value.\n",
    "\n",
    "in practice, this is usually a logistic function (for binary predictions), a softmax (for categorical predictions), a linearization-only node (for regression), or a collection of logistics (for multi-categoriy predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">what are your questions so far?</div>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### summary\n",
    "\n",
    "so a neural net is: a series of **layers**, where each **layer** is a stack of some number of **neurons**, and each **neuron** is a linearization (defined by a weight $w$ and a bias $b$) followed by an **activation** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### why it works\n",
    "\n",
    "if I just gave you a neural net with random number of layers, with layers of random node size, and with random weights and biases throughout, it would be *terrible* at making predictions. so it's not the *structure* that is making good predictions.\n",
    "\n",
    "rather, this particular way of arranging things has some special properties that make it easy to figure out how to tweak weights to incrementally improve those predictions. the process whereby we tweak weights is called *backpropagation* and is, at its heart, just the chain rule applied to millions of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "slowly but surely, and with enough input data, we can update the weights in our deep neural net to **learn** the ways of representing our data (the elements that come out of each layer of nodes) that are **optimal** for making our predictions.\n",
    "\n",
    "in a way, it's almost like cheating -- we know we want to make predictions, and we have a clever way of mashing together our input features such that what comes out is some $N$ dimensional vector that we can pass to a logistic regression and get amazing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### why we care (in this class)\n",
    "\n",
    "so why go through the hassle of covering this in \"advanced math and statistical computing\" when it's the topic of an entire different course?\n",
    "\n",
    "because there's so much computing action focused on deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we spent the bulk of last lecture talking about how anything that can be parallelized is a good candidate for `gpu` analytics and acceleration, and in particular linear algebra.\n",
    "\n",
    "well, as you saw above, deep neural nets are a giant pile of linear algebra. recent advancements in `gpu` availability (price and number) as well as speed have caused an explosion of `gpu` deep learning application development. including ours, in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## higher-level deep learning `api`s\n",
    "\n",
    "let's figure out how we can code deep neural network models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "recall the deep learning stack picture from above:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1M3LZQRI8nfCscnyL_h7xjKi4i9e8lo1t\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is called a stack because each level is providing a new interface and possibly new functionality \"on top of\" the level below it. the bottom three green levels are all very low level, and represent the particular stack made available by `nvidia`.\n",
    "\n",
    "the very lowest level of that diagram (`gpu`) is the hardware level. you could have several types of `gpu`, but in this class we will focuse on `nvidia` brand `gpu`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the level above that (`cuda`) is a set of `c++` libraries which allow programmers to write code that can be executed on the `gpu`. for `nvidia` `gpu`s, developers at `nvidia` have done the heavy lifting here, creating the bridge from the hardware (extremely low-level instructions!) to `c++` (a full OOP language).\n",
    "\n",
    "they have also used that set of `c++` libraries to create a second set of libraries that use the lower-level `c++` `cuda` code base to implement functionality that is specifically meant to be used in deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "at this point, if you are an extremely `l33t h4x0r` `c++` programmer, feel free to hop into your `ide` and bang out some deep neural nets.\n",
    "\n",
    "for the rest of us mere mortals, we will focus on even-higher-level apis in `python`. fortunately, a few exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the orange (`tensorflow`) and red (`keras`) boxes represent two levels of abstraction available to `python` coders for interacting with `gpu`s.\n",
    "\n",
    "+ `tensorflow` is a numerical computation framework that defines complicated computations as a directed graph of smaller computations\n",
    "    + we define a \"graph\" of operations (nodes) that we connect by their inputs and ouputs (edges)\n",
    "    + the graph defines how to get from one first node (e.g. loading the ultimate input) to any step downstream in the graph\n",
    "    + this is *not* deep learning specific (we could create another crappy alarm clock script with `tensorflow`, e.g.), but it was created with deep learning in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "perhaps that begs the question: how can I use this existing `c++` libraries from with `python`?\n",
    "\n",
    "you look for a `python` `api` that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ `keras` is a deep-learning-focused framework\n",
    "    + this is a high-level way of describing deep neural networks and training methods in simple `python` code\n",
    "    + it has *backends*, internal libraries which are used to *implement* the higher-level `keras` framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### deep learning `api`s\n",
    "\n",
    "each of these libraries is an *interface* to *something* beneath it. they provide developers a set of functions in some runtime (e.g. `python`) that hide the difficult, messy internal implementation details, so that someone who wants to use `tensorflow` to do *whatever* it is that `tensorflow` does won't need to know or care about what's happening a level below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "like any good interface, each of them **can** be an interface to the layer below\n",
    "\n",
    "+ `tensorflow` is a `python` interface to `c++` libraries `cuda` and `c++` (really, it \"goes through\" the `tensorflow` `c++` libraries, for an extra layer of interface-y goodness)\n",
    "+ `keras` is a `python` interface to `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "but they also **are not required** to be using that particular lower-level piece \n",
    "\n",
    "+ `tensorflow` can use non-`nvidia` or non-`gpu` lower-level libraries (that is, it can work on different `gpu`s, or `cpu`s, or google's proprietary `tpu`s, or `android` phones)\n",
    "+ `keras` can use other `python` deep learning libraries (e.g. `theano`, `cntk`, or apache `mxnet`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "you've used at least one subject-matter-specific `api` library before in this class: the `scikit-learn` library is a framework for creating machine learning models. you are used to relying on the same sort of `api` for `sklearn` models:\n",
    "\n",
    "```python\n",
    "model = sklearn.somemodeltype.MySpecialClassifier(param1, param2)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "model.predict(X_test)\n",
    "```\n",
    "\n",
    "all that changes from model to model is the actual `model` object you create, but there are standard ways of creating those models. then you assume they all have the same methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "finally, if you weren't confused enough yet, the `keras` library is available as a standalone `python` package but *also* as a modele within the `tensorflow` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package keras:\n",
      "\n",
      "NAME\n",
      "    keras\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    activations\n",
      "    applications (package)\n",
      "    backend (package)\n",
      "    callbacks\n",
      "    constraints\n",
      "    datasets (package)\n",
      "    engine (package)\n",
      "    initializers\n",
      "    layers (package)\n",
      "    legacy (package)\n",
      "    losses\n",
      "    metrics\n",
      "    models\n",
      "    objectives\n",
      "    optimizers\n",
      "    preprocessing (package)\n",
      "    regularizers\n",
      "    utils (package)\n",
      "    wrappers (package)\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "\n",
      "VERSION\n",
      "    2.2.4\n",
      "\n",
      "FILE\n",
      "    /Users/zach.lamberty/miniconda3/envs/bullshit/lib/python3.6/site-packages/keras/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "help(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow._api.v1.keras in tensorflow._api.v1:\n",
      "\n",
      "NAME\n",
      "    tensorflow._api.v1.keras - Implementation of the Keras API meant to be a high-level API for TensorFlow.\n",
      "\n",
      "DESCRIPTION\n",
      "    Detailed documentation and user guides are available at\n",
      "    [keras.io](https://keras.io).\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    activations (package)\n",
      "    applications (package)\n",
      "    backend (package)\n",
      "    callbacks (package)\n",
      "    constraints (package)\n",
      "    datasets (package)\n",
      "    estimator (package)\n",
      "    initializers (package)\n",
      "    layers (package)\n",
      "    losses (package)\n",
      "    metrics (package)\n",
      "    models (package)\n",
      "    optimizers (package)\n",
      "    preprocessing (package)\n",
      "    regularizers (package)\n",
      "    utils (package)\n",
      "    wrappers (package)\n",
      "\n",
      "VERSION\n",
      "    2.1.6-tf\n",
      "\n",
      "FILE\n",
      "    /Users/zach.lamberty/miniconda3/envs/bullshit/lib/python3.6/site-packages/tensorflow/_api/v1/keras/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help(tf.keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "note: these are not the same version!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's take a step back and re-focus on what we want to do, to help illuminate what these `api`s (`tensorflow` and `keras`) are doing for us.\n",
    "\n",
    "we want to create a deep neural network models, and we'd like to be able to use `gpu`s to accelerate our computation if we would like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`tensorflow` can help us do this\n",
    "\n",
    "+ **if** we can define our model as a directed graph of computation nodes and input / output edges, **then** `tensorflow` will handle the implementation on different lower-level hardwares (`gpu`, `cpu`, `tpu`, `android`) for us\n",
    "+ **if** we have a novel or experimental neural network architecture we want to try, **then** `tensorflow` provides us with all of the necessary infrastructure to create and train that model in the same way we would train any other model. we should be able to build pretty much *whatever* deep learning model we want\n",
    "+ **if** we want to train a fairly straightfward model type (`dnn`, `cnn`, `rnn`, `lstm`), **then** we may have to work a little bit harder than we'd like to define that model (see `keras`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "also, `keras` can help us do this\n",
    "\n",
    "+ **if** we have a backend which implements neural net computation methods (e.g. `tensorflow` or `theano`), **then** `keras` gives us a *much* simpler interface for writing that code\n",
    "+ **if** we want to use a different backend (`tensorflow` on one computer and `mxnet` on another), **then** `keras` will handle the implmenetation details for each without requiring code changes\n",
    "\n",
    "if you are just starting out and want to do some simple neural network development, I **strongly** encourage you to start with `keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "in particular, the author of the `keras` library (Franois Chollet) is a prolific author and blogger. his [`keras` blog](https://blog.keras.io/author/francois-chollet.html) is one of the best resources out there for tutorials on how to write deep learning models in `keras`. additionally, he wrote a great book: https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">what are your questions so far?</div>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### alternatives\n",
    "\n",
    "the *thing* `keras` gives us is a high-level backend-agnostic interface for creating most types of deep neural net architectures. alternative options include apache `mxnet`, which is a high-level framework with implementations in multiple different languages (and, coincidentally, one of the *backends* to `keras` to boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the *thing* `tensorflow` gives us is a computation environment with all the basic building blocks of deep neural net models (e.g. activation functions, loss functions, gradient descent algorithms) and supporting implementation on various different hardware types. the main alternative to `tensorflow` for this at this time is `pytorch`.\n",
    "\n",
    "many people prefer `pytorch` to `tensorflow`, so this is by no means a settled dispute. that being said, one of the people that prefers `tensorflow` is `google`, so I feel pretty confident that project will keep advancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## hands-on\n",
    "\n",
    "enough yaking, more key clacking. let's build some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">exercise: install `tensorflow` and `keras`</div>**\n",
    "\n",
    "on some machine where you have `conda` and some disk space, let's run\n",
    "\n",
    "```sh\n",
    "conda install -y tensorflow keras\n",
    "```\n",
    "\n",
    "verify it work by running (in a `python` or `ipython` session)\n",
    "\n",
    "```python\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "note: we also could have used `docker` to create a `container` with `tensorflow` (and therefore `keras`, via `tf.keras`) pre-installed. look at https://hub.docker.com/r/tensorflow/tensorflow/ for details, but the basic commands are\n",
    "\n",
    "```sh\n",
    "# pull (if you haven't) and run the latest py v3 tensorflow\n",
    "# container\n",
    "docker run --rm -it -p 8888:8888 tensorflow/tensorflow:latest-py3\n",
    "\n",
    "# open a jupyter notebook at localhost:8888\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### using `tensorflow`\n",
    "\n",
    "the [`tensorflow` documentation](https://www.tensorflow.org/tutorials/) is the definitive source for information on how to write `tensorflow` code, and this is no replacement. I simply want to cover the high-level concepts of working with `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### the execution graph\n",
    "\n",
    "the fundamental object in `tensorflow` is the execution graph: a directed graph connecting *computation* nodes (think `add`, `subtract`, `multiply`, etc) with edges that symoblize inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "you build this graph up as an object by adding an `add` operation to the graph. importantly, when you write\n",
    "\n",
    "```python\n",
    "mysum = tf.add(1, 1)\n",
    "```\n",
    "\n",
    "you **are not *performing* that computation**. you are creating an `add` operation object which has as its inputs two constant values (edges) with values of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mysum_op = tf.add(1, 1)\n",
    "mysum_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that operation was automatically added to the \"graph\" of computations as a single node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Add/x' type=Const>,\n",
       " <tf.Operation 'Add/y' type=Const>,\n",
       " <tf.Operation 'Add' type=Add>,\n",
       " <tf.Operation 'Add_1/x' type=Const>,\n",
       " <tf.Operation 'Add_1/y' type=Const>,\n",
       " <tf.Operation 'Add_1' type=Add>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.get_default_graph()\n",
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "if you actually want to get any information out of any operation you need to *evaluate* that node within a *session* -- you create a context in which `tensorflow` knows what inputs it should expect (you define them when you `run` the session!) and which outputs to return (you are `run`-ing operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    mysum_value = sess.run(mysum_op)\n",
    "mysum_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### eager execution\n",
    "\n",
    "when developing code, this extremely structured way of doing things (build a computation graph, then run it in a session) can be... pretty annoying.\n",
    "\n",
    "the google developers created an \"eager execution\" functionality to address exactly this problem. you can\n",
    "\n",
    "1. develop your code in \"eager execution\" mode -- get the results of your operation immediately\n",
    "1. remove one line of code from the beginning of your developed file and put everything inside a `tf.Session` to get the \"production\" behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15, shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE:\n",
    "# you must restart your kernel if you want to do this!!!\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "mysum_op = tf.add(1, 1)\n",
    "mysum_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysum_op.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ hands-on\n",
    "    + exercise: install tensorflow\n",
    "    + demo\n",
    "        + make a logistic classifier for iris\n",
    "        + at least in tf, maybe in all\n",
    "    + aws gpu instance\n",
    "        + nvidia has an available ami\n",
    "\n",
    "+ spec out the pricing of this!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
